/**
 * Swarm Clarifier
 *
 * Pre-flight clarification — asks up to 3 critical questions before execution.
 * If noQuestions is true, documents all assumptions without prompting.
 */

import type { LLMProvider } from "../providers/types.js";
import type { SwarmSpec } from "./spec-parser.js";

/**
 * A clarification question generated by the LLM
 */
export interface ClarificationQuestion {
  question: string;
  options?: string[];
  assumedAnswer: string;
  /** Confidence that the assumed answer is correct (0–1) */
  confidence: number;
  impact: "architecture" | "acceptance-criteria" | "scope";
}

/**
 * Result of the clarification step
 */
export interface ClarificationResult {
  spec: SwarmSpec;
  questions: ClarificationQuestion[];
  answers: Record<string, string>; // question text -> answer
  assumptions: string[]; // documented assumptions
  assumptionsFile: string; // path to .coco/swarm/assumptions.md
}

export interface ClarifyOptions {
  noQuestions?: boolean;
  /** @internal For testing: override the interactive prompt handler */
  _promptHandler?: (question: ClarificationQuestion) => Promise<string>;
}

/**
 * Run the clarification step.
 *
 * - If noQuestions is true: skip prompts, document all assumptions
 * - Otherwise: identify up to 3 critical ambiguities via LLM
 * - If confidence > 0.85: assume best option, document it
 * - Write .coco/swarm/assumptions.md
 */
export async function clarify(
  spec: SwarmSpec,
  projectPath: string,
  provider: LLMProvider,
  options: ClarifyOptions = {},
): Promise<ClarificationResult> {
  const questions = await identifyAmbiguities(spec, provider);

  const answers: Record<string, string> = {};
  const assumptions: string[] = [];

  if (options.noQuestions) {
    // Document all as assumptions
    for (const q of questions) {
      answers[q.question] = q.assumedAnswer;
      assumptions.push(
        `- **${q.impact}**: ${q.question}\n  → Assumed: "${q.assumedAnswer}" (confidence: ${Math.round(q.confidence * 100)}%)`,
      );
    }
  } else {
    // Ask low-confidence questions, assume high-confidence ones
    for (const q of questions) {
      if (q.confidence > 0.85) {
        // High confidence — assume without asking
        answers[q.question] = q.assumedAnswer;
        assumptions.push(
          `- **${q.impact}**: ${q.question}\n  → Auto-assumed: "${q.assumedAnswer}" (confidence: ${Math.round(q.confidence * 100)}%)`,
        );
      } else {
        // Ask the user (or use test handler)
        const handler = options._promptHandler ?? defaultPromptHandler;
        const answer = await handler(q);
        answers[q.question] = answer;
        if (answer === q.assumedAnswer) {
          assumptions.push(
            `- **${q.impact}**: ${q.question}\n  → Confirmed assumption: "${answer}"`,
          );
        } else {
          assumptions.push(`- **${q.impact}**: ${q.question}\n  → User clarified: "${answer}"`);
        }
      }
    }
  }

  const assumptionsFile = await writeAssumptionsFile(
    projectPath,
    spec.projectName,
    questions,
    assumptions,
  );

  return { spec, questions, answers, assumptions, assumptionsFile };
}

/**
 * Call the LLM to identify up to 3 critical ambiguities in the spec.
 */
async function identifyAmbiguities(
  spec: SwarmSpec,
  provider: LLMProvider,
): Promise<ClarificationQuestion[]> {
  const systemPrompt = `You are a product analyst identifying critical ambiguities in a software specification.
Your job is to find at most 3 questions that, if answered incorrectly, would significantly impact architecture, acceptance criteria, or scope.
Return ONLY a JSON array of question objects. No explanation, no markdown fences.`;

  const userMessage = `Analyze this specification and identify up to 3 critical ambiguities:

Project: ${spec.projectName}
Description: ${spec.description}
Tech Stack: ${JSON.stringify(spec.techStack)}
Features: ${spec.features.map((f) => `- ${f.name}: ${f.description}`).join("\n")}

Return a JSON array with this exact shape:
[
  {
    "question": "...",
    "options": ["option1", "option2"],
    "assumedAnswer": "...",
    "confidence": 0.0,
    "impact": "architecture|acceptance-criteria|scope"
  }
]

Rules:
- At most 3 questions
- Only include questions with HIGH impact if answered wrong
- confidence: 0.0-1.0 (how confident you are in the assumed answer)
- If the spec is clear enough, return []`;

  try {
    const response = await provider.chat([{ role: "user", content: userMessage }], {
      system: systemPrompt,
      maxTokens: 1024,
      temperature: 0.2,
    });

    const content = response.content.trim();
    // Strip markdown code fences if present
    const jsonStr = content.replace(/^```(?:json)?\s*\n?/, "").replace(/\n?```\s*$/, "");
    const parsed = JSON.parse(jsonStr) as ClarificationQuestion[];

    // Validate and clamp
    return parsed.slice(0, 3).map((q) => ({
      question: String(q.question || ""),
      options: Array.isArray(q.options) ? q.options.map(String) : undefined,
      assumedAnswer: String(q.assumedAnswer || ""),
      confidence: Math.min(1, Math.max(0, Number(q.confidence) || 0.5)),
      impact: (["architecture", "acceptance-criteria", "scope"].includes(q.impact)
        ? q.impact
        : "scope") as ClarificationQuestion["impact"],
    }));
  } catch {
    // LLM failed — return no questions (safe default)
    return [];
  }
}

/**
 * Default interactive prompt handler using @clack/prompts
 */
async function defaultPromptHandler(q: ClarificationQuestion): Promise<string> {
  const p = await import("@clack/prompts");

  if (q.options && q.options.length > 0) {
    const result = await p.select({
      message: q.question,
      options: [
        ...q.options.map((o) => ({ value: o, label: o })),
        { value: q.assumedAnswer, label: `${q.assumedAnswer} (default)` },
      ],
    });
    if (p.isCancel(result)) {
      return q.assumedAnswer;
    }
    return result as string;
  } else {
    const result = await p.text({
      message: q.question,
      placeholder: q.assumedAnswer,
    });
    if (p.isCancel(result) || !result) {
      return q.assumedAnswer;
    }
    return result as string;
  }
}

/**
 * Write the assumptions file to .coco/swarm/assumptions.md
 */
async function writeAssumptionsFile(
  projectPath: string,
  projectName: string,
  questions: ClarificationQuestion[],
  assumptions: string[],
): Promise<string> {
  const fs = await import("node:fs/promises");
  const path = await import("node:path");

  const swarmDir = path.join(projectPath, ".coco", "swarm");
  const assumptionsPath = path.join(swarmDir, "assumptions.md");

  await fs.mkdir(swarmDir, { recursive: true });

  const now = new Date().toISOString();
  const content = [
    `# Swarm Assumptions — ${projectName}`,
    ``,
    `Generated: ${now}`,
    ``,
    `## Summary`,
    ``,
    questions.length === 0
      ? `No ambiguities found. Proceeding with specification as-is.`
      : `${questions.length} question(s) identified. See details below.`,
    ``,
    `## Assumptions & Answers`,
    ``,
    assumptions.length > 0 ? assumptions.join("\n\n") : `_(none)_`,
    ``,
  ].join("\n");

  await fs.writeFile(assumptionsPath, content, "utf-8");
  return assumptionsPath;
}
